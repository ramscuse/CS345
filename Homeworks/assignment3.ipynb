{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS345 Fall 2022 Assignment 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearRegression\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "Datasets:\n",
    "\n",
    "* The [QSAR](http://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation) data for predicting the biochemical activity of a molecule.\n",
    "* The [Wisconsin breast cancer wisconsin dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer).\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  choosing optimal hyperparameters\n",
    "\n",
    "Just about any machine learning algorithm has some **hyperparameters**.  These are parameters that are set by the user and are not determined as part of the training process.\n",
    "The perceptron for example, has two of those - the number of epochs and the learning rate.  For the k-nearest neighbor classifier (kNN) it's the number of neighbors, $k$, and for the linear SVM it's the soft margin constant, $C$.  Our objective in machine learning is to obtain classifiers with high accuracy, and have good estimates of how well they are performing.  In other words, we need to know how accurate a classifier would be on unseen data.  This is why we use separate test sets that the classifier has not seen for evaluating accuracy.\n",
    "\n",
    "When working with classifiers with hyperparameters you may be tempted to apply the following procedure:\n",
    "\n",
    "* Randomly split the data into separate train and test sets.\n",
    "* Loop over a list of candidate values for the hyperparameter.\n",
    "* For each value, train the classifier over the training set and evaluate its performance on the test set.\n",
    "* Choose the parameter value that maximizes the accuracy over the test set, and report the accuracy that you obtained.\n",
    "\n",
    "However, it turns out that this procedure is flawed, and the resulting accuracy estimate can be overly optimistic.  This is because the choice of the best performing parameter value used information about the test set: by selecting the best value we used information about the labels of the test set.  Therefore, the resulting accuracy estimate uses the labels of the test set, invalidating this accuracy estimate as being totally without any knowledge of the test set.\n",
    "\n",
    "Here is a better approach.  Rather than splitting the data into train and test sets, we will now split the data into three sets:  **train, validation, and test**.  The validation set will be used for evaluation of different values of the hyperparameter, leading to the following approach:\n",
    "\n",
    "* Randomly split the data into separate train, validation, and test sets (say with ratios of 0.5, 0.2, 0.3).\n",
    "* Loop over a list of candidate values for the hyperparameter.\n",
    "* For each value, train the classifier over the **training set** and evaluate its performance on the **validation set**. \n",
    "* Choose the best classifier, and report its accuracy over the **test set**.\n",
    "\n",
    "Your task is as follows:\n",
    "\n",
    "* Use the method described above to evaluate the performance of the kNN classifier over the QSAR and Wisconsin breast cancer dataset.  Use a wide range of $k$ values.  Repeat the process ten times over different train/test splits and report the average accuracy over the test set.  What value of $k$ was chosen?  Note that the optimal value of $k$ may vary for different splits.  Comment on your results.\n",
    "\n",
    "* Perform the same experiment for the linear SVM. In this case the soft-margin constant $C$ is the hyperparameter that requires an informed choice.  Use a wide range of values for $C$, as we have done in class.  Comment on your results.\n",
    "\n",
    "You may use the scikit-learn kNN and SVM implementations and `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "QSAT_URL = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv\"\n",
    "QSATdata = pd.read_csv(QSAT_URL, sep=';', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.375, random_state=1)\n",
    "\n",
    "neigh = KNeighborsRegressor(n_neighbors=2)\n",
    "neigh.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**and space for discussing your results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  PCA for removing noise from data\n",
    "\n",
    "As we have seen in class, performance of the nearest neighbor classifer degrades when the data has noisy features that are not relevant to the classification problem.  To remedy this problem, we will use PCA to reduce the dimensionality of the data.\n",
    "\n",
    "Here is your task:\n",
    "\n",
    "* Use the QSAR dataset and evaluate performance of K nearest neighbors and SVM.  For simplicity, choose the values of K and $C$ that you selected in part 1.  Add 2000 noise features and evaluate model performance after doing so.\n",
    "* Next, use PCA to represent the data in the space of a given number of principal components.  Evaluate the performance of the KNN and SVM classifiers as you vary the number of principal components (no need to go above the original dimensionality of the dataset when doing so).  Plot the accuracy of each classifier as you vary the number of components.\n",
    "* Discuss your results:  was PCA useful for improving classifier performance?  Which of the two classifiers appears to be more robust to noise?\n",
    "* In class your instructer noted that the data needs to be centered or standardized before applying PCA.  Do you observe a difference in classifier performance when you center or standardize the data before applying PCA?  (Recall that centering refers to subtracting the mean from each feature, making it so that each feature has a mean of 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Report\n",
    "\n",
    "Answer the questions in the cells reserved for that purpose.\n",
    "\n",
    "\n",
    "### Submission\n",
    "\n",
    "Submit your report as a Jupyter notebook via Canvas.  Running the notebook should generate all the plots in your notebook.\n",
    "\n",
    "### Grading \n",
    "\n",
    "Although we will not grade on a 100 pt scale, the following is a grading sheet that will help you:\n",
    "\n",
    "\n",
    "```\n",
    "Grading sheet for assignment 3\n",
    "\n",
    "Part 1:  45 points (5 pts for discussion).\n",
    "Part 2:  55 points (10 pts for discussion).\n",
    "```\n",
    "\n",
    "Grading should be based on the following criteria:\n",
    "\n",
    "  * Code correctness.\n",
    "  * Plots and other results are well formatted and easy to understand.\n",
    "  * Interesting and meaningful observations made where requested.\n",
    "  * Notebook is readable, well-organized, and concise.\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "97feb688b70790b3a36568db4150e7c0b6184ca81aa48ab1cffefce025dbdfee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
