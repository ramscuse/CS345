{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS345 Fall 2022 Assignment 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated 9/30/22 at 10:30am with more details of the heart dataset.\n",
    "\n",
    "### Preliminaries\n",
    "\n",
    "We'll start with a review of the notation used to represent a dataset. In supervised learning we work with a dataset of $N$ labeled examples: $\\mathcal{D} = \\{ (\\mathbf{x}_i, y_i) \\}_{i=1}^N$, where $\\mathbf{x}_i$ is a $d$-dimensional vector (we always use boldface to denote vectors), and $y_i$ is the label associated with $\\mathbf{x}_i$.  For the perceptron algorithm we used the labels $\\pm 1$, so make sure that is the case for the data you read in.\n",
    "\n",
    "Datasets:\n",
    "\n",
    "* The [Gisette](http://archive.ics.uci.edu/ml/datasets/Gisette) handwritten digit recognition dataset. For this dataset use the separately provided validation set for testing your classifiers.\n",
    "* The [QSAR](http://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation) data for predicting the biochemical activity of a molecule.\n",
    "* The [heart disease diagnosis](http://archive.ics.uci.edu/ml/datasets/Heart+Disease) dataset.\n",
    "* The [Wisconsin breast cancer wisconsin dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer).\n",
    "* For developing your code, you can use one of the scikit-learn datasets, such as the [make_classification](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification) toy dataset generator.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Variation on the perceptron algorithm \n",
    "\n",
    "In this part of the assignment you will work with the perceptron and an additional variant we will call the **ensemble perceptron**.\n",
    "It is created by averaging the votes of multiple perceptron models to create a prediction.\n",
    "Ensemble learning is a common theme in machine learning, and later in the semester we will see algorithms specifically designed for this purpose.  In this assignment we will create a very simple implementation of this idea.\n",
    "The idea leverages a phenomenon known as [the wisdom of the crowd](https://en.wikipedia.org/wiki/Wisdom_of_the_crowd), namely that a collection of predictions of diverse experts (or even non experts), can, when aggregated lead to incredibly accurate predictions.\n",
    "\n",
    "\n",
    "**The idea:**  Rather than using a single classifier for making a prediction, use an average of the predictions made by multiple perceptrons trained on the same dataset.  This can lead to more robust and accurate predictions.\n",
    "\n",
    "Here are the details of the training algorithm:\n",
    "\n",
    "**Ensemble perceptron**\n",
    "\n",
    "**Input:** number of perceptrons in the ensemble (`num_classifiers`).\n",
    "\n",
    "**Output:**  a list of perceptron classifiers.\n",
    "\n",
    "**Training:**\n",
    "* Train `num_classifiers` perceptrons, each with a different initial weight vector (this is important!!!).  Each perceptron will be trained until convergence, or until a fixed number of epochs has passed (recall that an epoch is a loop over all the training data).\n",
    "\n",
    "**prediction:**\n",
    "\n",
    "* **decision_function:** Let $f_i(\\mathbf{x})$ be the decision function of perceptron $i$.  Then the decision function of the ensemble is defined as \n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\frac{1}{\\mathrm{num\\_classifiers}} \\sum_{i=1}^{\\mathrm{num\\_classifiers}} f_i(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "* **predict:**  The predict function will return $\\mathrm{sign}(f(\\mathbf{x}))$ where $f(\\mathbf{x})$ is the value of the decision function defined above.\n",
    "\n",
    "\n",
    "\n",
    "Here's what you need to do:\n",
    "\n",
    "- Implement the ensemble perceptron as a Python class called `ensemble_perceptron` using the same interface used in the code provided for the perceptron algorithm, i.e. provides the same methods with the same signature.  In each case make sure that your implementation **includes a bias term as described in the perceptron notebook** where you will find guidance on how to add a bias term to an algorithm that is expressed without one.\n",
    "\n",
    "- Compare the performance of the ensemble perceptron with the regular perceptron on the QSAR and breast cancer diagnosis datasets. Do so by estimating the accuracy on a sample of the data that you reserve for testing (the test set).  In each case reserve  70% of the data for training, and 30% for testing.  To gain more confidence in our error estimates, repeat this experiment using 10 random splits of the data into training/test sets for each algorithm.  It is best to use the same train-test splits for each algorithm.  Report the average accuracy and its standard deviation in a nicely formatted table.  Is there a version of the perceptron that appears to perform better?   (In answering this, consider the differences in performance you observe in comparison to the standard deviation).  Make sure to let the algorithm run for a sufficient number of epochs.\n",
    "\n",
    "A note about the classifier API:  in this course we follow the scikit-learn classifier API, which requires that a classifier have the following methods (in addition to a constructor):\n",
    "\n",
    "* `fit(X, y)`:  trains a classifier using a feature matrix `X` and a labels vector `y`.\n",
    "* `predict(X)`:  given a feature matrix `X`, return a vector of labels for each feature vector represented by `X`.\n",
    "\n",
    "For those interested in more information about the scikit-learn API, here's a [link](https://scikit-learn.org/stable/developers/develop.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized with a zeros weight vector\n",
      "accuracy:  0.6432748538011696\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron :\n",
    "    \"\"\"An implementation of the perceptron algorithm.\n",
    "    Note that this implementation does not include a bias term\"\"\"\n",
    " \n",
    "    def __init__(self, iterations=100, learning_rate=0.2, \n",
    "                 plot_data=False, random_w=False, seed=42) :\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.plot_data = plot_data\n",
    "        if random_w :\n",
    "            rng = np.random.default_rng(42)\n",
    "            self.w = rng.uniform(-1 , 1, len(X[0]))\n",
    "            print(\"initialized with random weight vector\")\n",
    "        else :\n",
    "            self.w = np.zeros(len(X[0]))\n",
    "            print(\"initialized with a zeros weight vector\")\n",
    "  \n",
    "    def fit(self, X, y) :\n",
    "        \"\"\"\n",
    "        Train a classifier using the perceptron training algorithm.\n",
    "        After training the attribute 'w' will contain the perceptron weight vector.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    " \n",
    "        X : ndarray, shape (num_examples, n_features)\n",
    "        Training data.\n",
    " \n",
    "        y : ndarray, shape (n_examples,)\n",
    "        Array of labels.\n",
    " \n",
    "        \"\"\"\n",
    "        self.wold = self.w\n",
    "        converged = False\n",
    "        iteration = 0\n",
    "        while (not converged and iteration <= self.iterations) :\n",
    "            converged = True\n",
    "            for i in range(len(X)) :\n",
    "                if y[i] * self.decision_function(X[i]) <= 0 :\n",
    "                    self.wold = self.w\n",
    "                    self.w = self.w + y[i] * self.learning_rate * X[i]\n",
    "                    converged = False\n",
    "                    if self.plot_data:\n",
    "                        self.plot_update(X, y, i)\n",
    "            iteration += 1\n",
    "        self.converged = converged\n",
    "        if converged :\n",
    "            print ('converged in %d iterations ' % iteration)\n",
    " \n",
    "    def decision_function(self, x) :\n",
    "        return np.dot(x, self.w)\n",
    " \n",
    "    def predict(self, X) :\n",
    "        \"\"\"\n",
    "        make predictions using a trained linear classifier\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    " \n",
    "        X : ndarray, shape (num_examples, n_features)\n",
    "        Training data.\n",
    "        \"\"\"\n",
    " \n",
    "        scores = np.dot(X, self.w)\n",
    "        return np.sign(scores)\n",
    "    \n",
    "    def plot_update(self, X, y, ipt) :\n",
    "        fig = plt.figure(figsize=(4,4))\n",
    "        plt.xlim(-1,1)\n",
    "        plt.ylim(-1,1)\n",
    "        plt.xlabel(\"Feature 1\")\n",
    "        plt.ylabel(\"Feature 2\")\n",
    "        plt.arrow(0,0,self.w[0],self.w[1], \n",
    "                  width=0.001,head_width=0.05, \n",
    "                  length_includes_head=True, alpha=1,\n",
    "                  linestyle='-',color='darkred')\n",
    "        plt.arrow(0,0,self.wold[0],self.wold[1], \n",
    "                  width=0.001,head_width=0.05, \n",
    "                  length_includes_head=True, alpha=1,\n",
    "                  linestyle='-',color='orange')\n",
    "        anew = -self.w[0]/self.w[1]\n",
    "        aold = -self.wold[0]/self.wold[1]\n",
    "        pts = np.linspace(-1,1)\n",
    "        plt.plot(pts, anew*pts, color='darkred')\n",
    "        plt.plot(pts, aold*pts, color='orange')\n",
    "        plt.title(\"in orange:  old w; in red:  new w\")\n",
    "        cols = {1: 'g', -1: 'b'}\n",
    "        for i in range(len(X)): \n",
    "            plt.plot(X[i][0], X[i][1], cols[y[i]]+'o', alpha=0.6,markersize=5) \n",
    "        plt.plot(X[ipt][0], X[ipt][1], 'ro', alpha=0.2,markersize=20)\n",
    "\n",
    "\n",
    "class ensemble_perceptron :\n",
    "    perceptrons = []\n",
    "    \n",
    "    def __init__(self, num_classifiers=50, iterations=100, learning_rate=0.2) :\n",
    "        self.num_classifiers = num_classifiers\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.perceptrons = [Perceptron(iterations = self.iterations, learning_rate = self.learning_rate)] * num_classifiers\n",
    "    \n",
    "    def fit(self, X, y) :\n",
    "        for p in self.perceptrons :\n",
    "            p.fit(X,y)\n",
    "    \n",
    "    def predict(self, X) :\n",
    "        total = 0\n",
    "        for p in self.perceptrons :\n",
    "            total += p.predict(X)\n",
    "        return total / self.num_classifiers\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X,y = data = load_breast_cancer(return_X_y = True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)\n",
    "\n",
    "p = ensemble_perceptron(num_classifiers=50, iterations=100, learning_rate=0.2)\n",
    "\n",
    "p.fit(X_train, y_train)\n",
    "y_pred = p.predict(X_test)\n",
    "print('accuracy: ', np.mean(y_test == y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Learning Curves \n",
    "\n",
    "Whenever we train a classifier it is useful to know if we have collected a sufficient amount of data for accurate classification.  A good way of determining that is to construct a **learning curve**, which is a plot of classifier performance as a function of the number of training examples.  Plot a learning curve for the perceptron algorithm using the [Gisette](http://archive.ics.uci.edu/ml/datasets/Gisette) handwritten digit recognition dataset. For this dataset use the separately provided validation set for testing your classifiers.  A test set is provided without its labels, so is not usable for us.\n",
    "The x-axis for the plot (number of training examples) should be on a logarithmic scale - something like 10,20,40,80,200,400,800.  Use numbers that are appropriate for the dataset at hand, choosing values that illustrate the variation that you observe.  What can you conclude from the learning curve you have constructed for this particular dataset?\n",
    "In answering this question, you can use the following [wikipedia article](https://en.wikipedia.org/wiki/Learning_curve#In_machine_learning).\n",
    "Make sure that you use a fixed test set to evaluate performance while varying the size of the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "gtest_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/gisette/GISETTE/gisette_test.data\"\n",
    "gtrain_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/gisette/GISETTE/gisette_train.data\"\n",
    "glabel_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/gisette/GISETTE/gisette_train.labels\"\n",
    "\n",
    "gtest = np.genfromtxt(gtest_url, delimiter=' ')\n",
    "gtrain = np.genfromtxt(gtrain_url, delimiter=' ')\n",
    "glabel = np.genfromtxt(glabel_url, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6500, 5000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 5000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y=gtest[:,0]\n",
    "X=gtest[y>=0,1:]\n",
    "y=y[y>=0]\n",
    "scaler = StandardScaler(with_std=False).fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6500, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_components = 20\n",
    "pca = PCA(n_components=num_components)\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, num_components + 1), pca.explained_variance_ratio_, 'ob')\n",
    "plt.xlabel(\"component\")\n",
    "plt.xticks(range(1,num_components+1, 2))\n",
    "plt.ylabel(\"fraction of explained variance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=gtrain[:,0]\n",
    "X=gtrain[y>=0,1:]\n",
    "y=y[y>=0]\n",
    "scaler = StandardScaler(with_std=False).fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_components = 20\n",
    "pca = PCA(n_components=num_components)\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, num_components + 1), pca.explained_variance_ratio_, 'ob')\n",
    "plt.xlabel(\"component\")\n",
    "plt.xticks(range(1,num_components+1, 2))\n",
    "plt.ylabel(\"fraction of explained variance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*results and their discussion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Data standardization \n",
    "\n",
    "In this section we will explore the effect of normalizing the data, focusing on normalization of each feature individually.  In class we saw how to convert each column (i.e. feature) of a data matrix so that it fall in the range $[-1,1]$.  In this assignment we will explore a different approach callled **standardization**.\n",
    "\n",
    "Here's what you need to do:\n",
    "\n",
    "* Write a method to standardize a data matrix, so that each column has zero mean and standard deviation equal to 1.  This is done by subtracting the mean of each column, and dividing by its standard deviation.  See details [here](https://en.wikipedia.org/wiki/Feature_scaling#Standardization_(Z-score_Normalization)).  \n",
    "Do not use the scikit-learn implementation.\n",
    "\n",
    "* Compare the accuracy of the standard perceptron on the heart dataset  with standardization and without it.  Which leads to better performance?  Can you explain why?\n",
    "\n",
    "\n",
    "In this exercise we're using the\n",
    "[Heart disease diagnosis dataset](http://archive.ics.uci.edu/ml/datasets/Heart+Disease).\n",
    "This dataset has several data files associated with it.  The easiest would be to use [this file](http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data), where categorical variables have been replaced with numerical vaues.  The last column in the file contains the label associated with each example.  In the processed file, a label `0` corresponds to a healthy individual; other values correspond to varying levels of heart disease.  **In your experiments focus on the binary classification problem of trying to distinguish between healthy and non-healthy individuals.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>63.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>1.0.1</th>\n",
       "      <th>145.0</th>\n",
       "      <th>233.0</th>\n",
       "      <th>1.0.2</th>\n",
       "      <th>2.0</th>\n",
       "      <th>150.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>2.3</th>\n",
       "      <th>3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.392608</td>\n",
       "      <td>0.686734</td>\n",
       "      <td>0.875028</td>\n",
       "      <td>1.609924</td>\n",
       "      <td>0.757115</td>\n",
       "      <td>-0.412284</td>\n",
       "      <td>1.018423</td>\n",
       "      <td>-1.815827</td>\n",
       "      <td>1.429586</td>\n",
       "      <td>0.400241</td>\n",
       "      <td>0.660153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.346285</td>\n",
       "      <td>0.689494</td>\n",
       "      <td>0.878658</td>\n",
       "      <td>-0.583376</td>\n",
       "      <td>-0.312711</td>\n",
       "      <td>-0.406857</td>\n",
       "      <td>1.023458</td>\n",
       "      <td>-0.829649</td>\n",
       "      <td>1.418468</td>\n",
       "      <td>1.350758</td>\n",
       "      <td>0.665321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.704639</td>\n",
       "      <td>0.692242</td>\n",
       "      <td>-0.149502</td>\n",
       "      <td>-0.035207</td>\n",
       "      <td>0.089415</td>\n",
       "      <td>-0.401565</td>\n",
       "      <td>-0.988711</td>\n",
       "      <td>1.476891</td>\n",
       "      <td>-0.695379</td>\n",
       "      <td>2.135752</td>\n",
       "      <td>2.294650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.233419</td>\n",
       "      <td>-1.448186</td>\n",
       "      <td>-1.148902</td>\n",
       "      <td>-0.013339</td>\n",
       "      <td>-0.701789</td>\n",
       "      <td>-0.396402</td>\n",
       "      <td>1.026769</td>\n",
       "      <td>0.877668</td>\n",
       "      <td>-0.685807</td>\n",
       "      <td>0.327549</td>\n",
       "      <td>-0.956154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.210989</td>\n",
       "      <td>0.684435</td>\n",
       "      <td>-1.106690</td>\n",
       "      <td>-0.426995</td>\n",
       "      <td>-0.127709</td>\n",
       "      <td>-0.391362</td>\n",
       "      <td>-0.979077</td>\n",
       "      <td>1.070106</td>\n",
       "      <td>-0.676571</td>\n",
       "      <td>-0.190768</td>\n",
       "      <td>-0.920942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.750806</td>\n",
       "      <td>-1.394374</td>\n",
       "      <td>0.850161</td>\n",
       "      <td>0.434123</td>\n",
       "      <td>0.416715</td>\n",
       "      <td>-0.386440</td>\n",
       "      <td>1.030133</td>\n",
       "      <td>0.438359</td>\n",
       "      <td>-0.667651</td>\n",
       "      <td>2.240678</td>\n",
       "      <td>2.219387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.315011</td>\n",
       "      <td>-1.348322</td>\n",
       "      <td>0.854561</td>\n",
       "      <td>-0.354660</td>\n",
       "      <td>1.803892</td>\n",
       "      <td>-0.381631</td>\n",
       "      <td>-0.969657</td>\n",
       "      <td>0.536498</td>\n",
       "      <td>1.390193</td>\n",
       "      <td>-0.359465</td>\n",
       "      <td>-0.890791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.817476</td>\n",
       "      <td>0.671153</td>\n",
       "      <td>0.859035</td>\n",
       "      <td>0.052895</td>\n",
       "      <td>0.210033</td>\n",
       "      <td>-0.376929</td>\n",
       "      <td>1.033549</td>\n",
       "      <td>0.032061</td>\n",
       "      <td>-0.658551</td>\n",
       "      <td>0.340898</td>\n",
       "      <td>0.668059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005890</td>\n",
       "      <td>0.673813</td>\n",
       "      <td>0.863581</td>\n",
       "      <td>0.430832</td>\n",
       "      <td>-0.568436</td>\n",
       "      <td>2.380512</td>\n",
       "      <td>1.038584</td>\n",
       "      <td>0.288762</td>\n",
       "      <td>1.377310</td>\n",
       "      <td>1.824034</td>\n",
       "      <td>2.198914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.332074</td>\n",
       "      <td>0.676462</td>\n",
       "      <td>0.868198</td>\n",
       "      <td>0.432569</td>\n",
       "      <td>-0.712029</td>\n",
       "      <td>-0.365686</td>\n",
       "      <td>-0.958975</td>\n",
       "      <td>0.088908</td>\n",
       "      <td>-0.649896</td>\n",
       "      <td>-0.525674</td>\n",
       "      <td>0.681687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.261463</td>\n",
       "      <td>-1.304031</td>\n",
       "      <td>-0.987751</td>\n",
       "      <td>0.434764</td>\n",
       "      <td>0.819989</td>\n",
       "      <td>-0.361553</td>\n",
       "      <td>1.042044</td>\n",
       "      <td>0.243326</td>\n",
       "      <td>-0.641916</td>\n",
       "      <td>0.263688</td>\n",
       "      <td>0.686880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.268586</td>\n",
       "      <td>0.672840</td>\n",
       "      <td>-0.047206</td>\n",
       "      <td>0.106403</td>\n",
       "      <td>0.267366</td>\n",
       "      <td>2.244078</td>\n",
       "      <td>1.047088</td>\n",
       "      <td>-0.055227</td>\n",
       "      <td>1.362082</td>\n",
       "      <td>-0.343974</td>\n",
       "      <td>0.692083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.593424</td>\n",
       "      <td>0.675450</td>\n",
       "      <td>-0.936335</td>\n",
       "      <td>-0.204432</td>\n",
       "      <td>0.373990</td>\n",
       "      <td>-0.353835</td>\n",
       "      <td>-0.948557</td>\n",
       "      <td>0.799451</td>\n",
       "      <td>-0.633950</td>\n",
       "      <td>-0.862761</td>\n",
       "      <td>-0.828662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.003320</td>\n",
       "      <td>0.678050</td>\n",
       "      <td>-0.028236</td>\n",
       "      <td>1.442282</td>\n",
       "      <td>-0.515148</td>\n",
       "      <td>2.144055</td>\n",
       "      <td>-0.941207</td>\n",
       "      <td>0.504844</td>\n",
       "      <td>-0.626475</td>\n",
       "      <td>-0.423790</td>\n",
       "      <td>-0.803066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.354802</td>\n",
       "      <td>0.680640</td>\n",
       "      <td>-0.019167</td>\n",
       "      <td>0.754666</td>\n",
       "      <td>-0.923068</td>\n",
       "      <td>-0.347967</td>\n",
       "      <td>-0.934016</td>\n",
       "      <td>0.818214</td>\n",
       "      <td>-0.619210</td>\n",
       "      <td>0.532153</td>\n",
       "      <td>-0.779291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.250089</td>\n",
       "      <td>0.683220</td>\n",
       "      <td>-0.871526</td>\n",
       "      <td>-0.446621</td>\n",
       "      <td>-0.076202</td>\n",
       "      <td>-0.344449</td>\n",
       "      <td>-0.926978</td>\n",
       "      <td>0.663790</td>\n",
       "      <td>-0.612145</td>\n",
       "      <td>0.015744</td>\n",
       "      <td>2.126399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.164382</td>\n",
       "      <td>0.685790</td>\n",
       "      <td>0.844564</td>\n",
       "      <td>0.453385</td>\n",
       "      <td>0.068150</td>\n",
       "      <td>-0.340990</td>\n",
       "      <td>-0.920086</td>\n",
       "      <td>0.465449</td>\n",
       "      <td>-0.605269</td>\n",
       "      <td>0.191541</td>\n",
       "      <td>-0.757698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.218486</td>\n",
       "      <td>-1.260944</td>\n",
       "      <td>0.006704</td>\n",
       "      <td>0.170192</td>\n",
       "      <td>0.548839</td>\n",
       "      <td>-0.337588</td>\n",
       "      <td>-0.913335</td>\n",
       "      <td>-0.042763</td>\n",
       "      <td>-0.598572</td>\n",
       "      <td>-0.670077</td>\n",
       "      <td>-0.736698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.140240</td>\n",
       "      <td>0.682767</td>\n",
       "      <td>-0.819434</td>\n",
       "      <td>0.178834</td>\n",
       "      <td>0.436464</td>\n",
       "      <td>-0.334241</td>\n",
       "      <td>-0.906720</td>\n",
       "      <td>0.734593</td>\n",
       "      <td>-0.592046</td>\n",
       "      <td>-0.321002</td>\n",
       "      <td>-0.716954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.816505</td>\n",
       "      <td>0.685300</td>\n",
       "      <td>-1.620945</td>\n",
       "      <td>-0.364269</td>\n",
       "      <td>-0.258215</td>\n",
       "      <td>-0.330947</td>\n",
       "      <td>1.042584</td>\n",
       "      <td>0.100419</td>\n",
       "      <td>1.336588</td>\n",
       "      <td>0.714823</td>\n",
       "      <td>0.684558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.445245</td>\n",
       "      <td>-1.226009</td>\n",
       "      <td>-1.583580</td>\n",
       "      <td>0.734966</td>\n",
       "      <td>0.657322</td>\n",
       "      <td>2.057158</td>\n",
       "      <td>1.047540</td>\n",
       "      <td>0.525592</td>\n",
       "      <td>-0.585740</td>\n",
       "      <td>0.029541</td>\n",
       "      <td>-0.691375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.450005</td>\n",
       "      <td>0.682957</td>\n",
       "      <td>-0.756821</td>\n",
       "      <td>-0.062549</td>\n",
       "      <td>0.672880</td>\n",
       "      <td>-0.327097</td>\n",
       "      <td>1.052509</td>\n",
       "      <td>0.483801</td>\n",
       "      <td>-0.579557</td>\n",
       "      <td>0.720465</td>\n",
       "      <td>0.687562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.454786</td>\n",
       "      <td>0.685454</td>\n",
       "      <td>0.041865</td>\n",
       "      <td>0.262804</td>\n",
       "      <td>-0.057791</td>\n",
       "      <td>-0.324007</td>\n",
       "      <td>1.057491</td>\n",
       "      <td>0.779017</td>\n",
       "      <td>-0.573521</td>\n",
       "      <td>1.929410</td>\n",
       "      <td>-0.667284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.578123</td>\n",
       "      <td>0.687941</td>\n",
       "      <td>0.824142</td>\n",
       "      <td>0.218296</td>\n",
       "      <td>-0.265458</td>\n",
       "      <td>-0.320962</td>\n",
       "      <td>1.062486</td>\n",
       "      <td>-0.123968</td>\n",
       "      <td>1.324208</td>\n",
       "      <td>1.250600</td>\n",
       "      <td>0.690863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.002870</td>\n",
       "      <td>-1.192986</td>\n",
       "      <td>0.057067</td>\n",
       "      <td>-0.027528</td>\n",
       "      <td>-0.099001</td>\n",
       "      <td>-0.317963</td>\n",
       "      <td>-0.893090</td>\n",
       "      <td>0.452365</td>\n",
       "      <td>-0.567773</td>\n",
       "      <td>0.563069</td>\n",
       "      <td>0.695930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.467979</td>\n",
       "      <td>-1.165594</td>\n",
       "      <td>0.064076</td>\n",
       "      <td>-0.017196</td>\n",
       "      <td>1.336748</td>\n",
       "      <td>-0.315006</td>\n",
       "      <td>-0.886768</td>\n",
       "      <td>0.757197</td>\n",
       "      <td>-0.562031</td>\n",
       "      <td>-0.822162</td>\n",
       "      <td>-0.638074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.928181</td>\n",
       "      <td>-1.140267</td>\n",
       "      <td>-1.449661</td>\n",
       "      <td>0.730137</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>-0.312092</td>\n",
       "      <td>-0.880568</td>\n",
       "      <td>-0.467324</td>\n",
       "      <td>-0.556416</td>\n",
       "      <td>1.432364</td>\n",
       "      <td>2.021997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.364602</td>\n",
       "      <td>0.679135</td>\n",
       "      <td>0.825392</td>\n",
       "      <td>0.732545</td>\n",
       "      <td>0.257569</td>\n",
       "      <td>-0.309220</td>\n",
       "      <td>-0.874483</td>\n",
       "      <td>0.736507</td>\n",
       "      <td>-0.550924</td>\n",
       "      <td>0.485351</td>\n",
       "      <td>-0.622412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.518772</td>\n",
       "      <td>0.681554</td>\n",
       "      <td>0.830908</td>\n",
       "      <td>-0.222798</td>\n",
       "      <td>-0.650084</td>\n",
       "      <td>-0.306387</td>\n",
       "      <td>1.062931</td>\n",
       "      <td>-0.436181</td>\n",
       "      <td>1.311919</td>\n",
       "      <td>0.922418</td>\n",
       "      <td>0.706580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.087212</td>\n",
       "      <td>-1.114606</td>\n",
       "      <td>-1.399145</td>\n",
       "      <td>0.497710</td>\n",
       "      <td>0.177237</td>\n",
       "      <td>-0.303594</td>\n",
       "      <td>-0.867111</td>\n",
       "      <td>0.330512</td>\n",
       "      <td>-0.545788</td>\n",
       "      <td>0.752950</td>\n",
       "      <td>-0.601799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        63.0       1.0     1.0.1     145.0     233.0     1.0.2       2.0  \\\n",
       "0   1.392608  0.686734  0.875028  1.609924  0.757115 -0.412284  1.018423   \n",
       "1   1.346285  0.689494  0.878658 -0.583376 -0.312711 -0.406857  1.023458   \n",
       "2  -1.704639  0.692242 -0.149502 -0.035207  0.089415 -0.401565 -0.988711   \n",
       "3  -1.233419 -1.448186 -1.148902 -0.013339 -0.701789 -0.396402  1.026769   \n",
       "4   0.210989  0.684435 -1.106690 -0.426995 -0.127709 -0.391362 -0.979077   \n",
       "5   0.750806 -1.394374  0.850161  0.434123  0.416715 -0.386440  1.030133   \n",
       "6   0.315011 -1.348322  0.854561 -0.354660  1.803892 -0.381631 -0.969657   \n",
       "7   0.817476  0.671153  0.859035  0.052895  0.210033 -0.376929  1.033549   \n",
       "8   0.005890  0.673813  0.863581  0.430832 -0.568436  2.380512  1.038584   \n",
       "9   0.332074  0.676462  0.868198  0.432569 -0.712029 -0.365686 -0.958975   \n",
       "10  0.261463 -1.304031 -0.987751  0.434764  0.819989 -0.361553  1.042044   \n",
       "11  0.268586  0.672840 -0.047206  0.106403  0.267366  2.244078  1.047088   \n",
       "12 -0.593424  0.675450 -0.936335 -0.204432  0.373990 -0.353835 -0.948557   \n",
       "13 -0.003320  0.678050 -0.028236  1.442282 -0.515148  2.144055 -0.941207   \n",
       "14  0.354802  0.680640 -0.019167  0.754666 -0.923068 -0.347967 -0.934016   \n",
       "15 -0.250089  0.683220 -0.871526 -0.446621 -0.076202 -0.344449 -0.926978   \n",
       "16  0.164382  0.685790  0.844564  0.453385  0.068150 -0.340990 -0.920086   \n",
       "17 -0.218486 -1.260944  0.006704  0.170192  0.548839 -0.337588 -0.913335   \n",
       "18 -0.140240  0.682767 -0.819434  0.178834  0.436464 -0.334241 -0.906720   \n",
       "19  0.816505  0.685300 -1.620945 -0.364269 -0.258215 -0.330947  1.042584   \n",
       "20  0.445245 -1.226009 -1.583580  0.734966  0.657322  2.057158  1.047540   \n",
       "21  0.450005  0.682957 -0.756821 -0.062549  0.672880 -0.327097  1.052509   \n",
       "22  0.454786  0.685454  0.041865  0.262804 -0.057791 -0.324007  1.057491   \n",
       "23  0.578123  0.687941  0.824142  0.218296 -0.265458 -0.320962  1.062486   \n",
       "24 -0.002870 -1.192986  0.057067 -0.027528 -0.099001 -0.317963 -0.893090   \n",
       "25  0.467979 -1.165594  0.064076 -0.017196  1.336748 -0.315006 -0.886768   \n",
       "26  0.928181 -1.140267 -1.449661  0.730137  0.006390 -0.312092 -0.880568   \n",
       "27 -0.364602  0.679135  0.825392  0.732545  0.257569 -0.309220 -0.874483   \n",
       "28 -0.518772  0.681554  0.830908 -0.222798 -0.650084 -0.306387  1.062931   \n",
       "29  1.087212 -1.114606 -1.399145  0.497710  0.177237 -0.303594 -0.867111   \n",
       "\n",
       "       150.0       0.0       2.3       3.0  \n",
       "0  -1.815827  1.429586  0.400241  0.660153  \n",
       "1  -0.829649  1.418468  1.350758  0.665321  \n",
       "2   1.476891 -0.695379  2.135752  2.294650  \n",
       "3   0.877668 -0.685807  0.327549 -0.956154  \n",
       "4   1.070106 -0.676571 -0.190768 -0.920942  \n",
       "5   0.438359 -0.667651  2.240678  2.219387  \n",
       "6   0.536498  1.390193 -0.359465 -0.890791  \n",
       "7   0.032061 -0.658551  0.340898  0.668059  \n",
       "8   0.288762  1.377310  1.824034  2.198914  \n",
       "9   0.088908 -0.649896 -0.525674  0.681687  \n",
       "10  0.243326 -0.641916  0.263688  0.686880  \n",
       "11 -0.055227  1.362082 -0.343974  0.692083  \n",
       "12  0.799451 -0.633950 -0.862761 -0.828662  \n",
       "13  0.504844 -0.626475 -0.423790 -0.803066  \n",
       "14  0.818214 -0.619210  0.532153 -0.779291  \n",
       "15  0.663790 -0.612145  0.015744  2.126399  \n",
       "16  0.465449 -0.605269  0.191541 -0.757698  \n",
       "17 -0.042763 -0.598572 -0.670077 -0.736698  \n",
       "18  0.734593 -0.592046 -0.321002 -0.716954  \n",
       "19  0.100419  1.336588  0.714823  0.684558  \n",
       "20  0.525592 -0.585740  0.029541 -0.691375  \n",
       "21  0.483801 -0.579557  0.720465  0.687562  \n",
       "22  0.779017 -0.573521  1.929410 -0.667284  \n",
       "23 -0.123968  1.324208  1.250600  0.690863  \n",
       "24  0.452365 -0.567773  0.563069  0.695930  \n",
       "25  0.757197 -0.562031 -0.822162 -0.638074  \n",
       "26 -0.467324 -0.556416  1.432364  2.021997  \n",
       "27  0.736507 -0.550924  0.485351 -0.622412  \n",
       "28 -0.436181  1.311919  0.922418  0.706580  \n",
       "29  0.330512 -0.545788  0.752950 -0.601799  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('processed.cleveland.data')\n",
    "data = data.iloc[: , :-3]\n",
    "\n",
    "for col in data.columns :\n",
    "    for i in range(data.shape[0]) :\n",
    "        data.at[i,col] = (data.at[i,col] - data[col].mean()) / data[col].std()\n",
    "        \n",
    "data.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReRun the cell below. For whatever reason it does not like whe it is first inialized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized with a zeros weight vector\n",
      "accuracy:  0.0033112582781456954\n",
      "initialized with a zeros weight vector\n",
      "accuracy:  0.0033112582781456954\n"
     ]
    }
   ],
   "source": [
    "p = Perceptron()\n",
    "X = data.to_numpy()\n",
    "y = np.array(np.arange(X.shape[0]))\n",
    "p.fit(X,y)\n",
    "y_pred = p.predict(X)\n",
    "print('accuracy: ', np.mean(y == y_pred))\n",
    "\n",
    "p1 = Perceptron()\n",
    "data1 = pd.read_csv('processed.cleveland.data')\n",
    "data1 = data1.iloc[: , :-3]\n",
    "X1 = data1.to_numpy()\n",
    "y1 = np.array(np.arange(X1.shape[0]))\n",
    "p1.fit(X1,y1)\n",
    "y1_pred = p.predict(X1)\n",
    "print('accuracy: ', np.mean(y1 == y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be no difference in accuracy between the two predictions. This is because when you standardize data, the data doesn't change. All that changes is the units attached. Every point becomes a Z score which tells us how many standard deviations away from the mean the point is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Report\n",
    "\n",
    "Answer the questions in the cells reserved for that purpose.\n",
    "\n",
    "\n",
    "### Submission\n",
    "\n",
    "Submit your report as a Jupyter notebook via Canvas.  Running the notebook should generate all the plots in your notebook.\n",
    "\n",
    "### Grading \n",
    "\n",
    "Although we will not grade on a 100 pt scale, the following is a grading sheet that will help you:\n",
    "\n",
    "```\n",
    "Grading sheet for assignment 2\n",
    "\n",
    "Part 1:  60 points.\n",
    "Part 2:  20 points.\n",
    "Part 3:  20 points.\n",
    "```\n",
    "\n",
    "Grading will be based on the following criteria:\n",
    "\n",
    "  * Code correctness.\n",
    "  * Plots and other results are well formatted and easy to understand.\n",
    "  * Interesting and meaningful observations made where requested.\n",
    "  * Notebook is readable, well-organized, and concise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
